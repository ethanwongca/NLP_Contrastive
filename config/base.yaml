model:
  class_path: encoders.experiment.VideoTextExp
  init_args:
    initial_lr: 0.0001
    weight_decay: 0.001
    num_warmup_steps: 500
    optimizer: Adam8bit
    hard_negatives: true
    #tokenizer: Qwen2VLProcessor (I don't quite understand here, don't the text encoder and video encoder have their own tokenizers? Why do we need to initialize one for the whole class?)
    #processor: Qwen2VLProcessor (same question as the tokenizer)
    #zeroshot: false (we don't need this since we don't have a zeroshot eval function?)

    text_encoder_cfg:
      model_name: jinaai/jina-embeddings-v3
      task: text-matching
      out_hidden_size: 1024  # The matryoshka dimension

    video_encoder_cfg:
      model_name: Qwen/Qwen2.5-VL-3B-Instruct
      depth: 32
      fullatt_block_indexes: [7, 15, 23, 31]
      hidden_act: silu
      hidden_size: 1024  
      in_channels: 3
      intermediate_size: 4096
      model_type: qwen2_5_vl
      num_heads: 16
      out_hidden_size: 1024
      patch_size: 14
      spatial_merge_size: 2
      temporal_patch_size: 2
      tokens_per_second: 4
      transformers_version: 4.49.0
      window_size: 112
      _attn_implementation: eager
      proj_size: 512
      torch_dtype: bfloat16
        
