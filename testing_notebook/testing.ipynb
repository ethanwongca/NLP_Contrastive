{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3047479d-fa5f-4e57-ae31-482d5d510370",
   "metadata": {},
   "source": [
    "# Video Encoder testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a9fa8-22d5-48b9-b063-f6733e6e593f",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e181d-9ba8-4061-ae7a-36c5db864d42",
   "metadata": {},
   "source": [
    "### Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d6040-dbe5-4b8f-b0f1-3c52f59e9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Qwen2VLProcessor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.jit\n",
    "\n",
    "from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLConfig\n",
    "from transformers.models.qwen2_5_vl.modular_qwen2_5_vl import (\n",
    "    Qwen2_5_VLPreTrainedModel ,\n",
    "    Qwen2_5_VLVisionConfig,\n",
    "    Qwen2_5_VisionPatchEmbed,\n",
    "    Qwen2_5_VisionRotaryEmbedding,\n",
    "    Qwen2_5_VLVisionBlock,\n",
    "    Qwen2_5_VLPatchMerger\n",
    ")\n",
    "\n",
    "class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):\n",
    "    config_class = Qwen2_5_VLVisionConfig\n",
    "    _no_split_modules = [\"Qwen2_5_VLVisionBlock\"]\n",
    "\n",
    "    def __init__(self, config: Qwen2_5_VLVisionConfig, *inputs, **kwargs) -> None:\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.spatial_merge_size = config.spatial_merge_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.fullatt_block_indexes = config.fullatt_block_indexes\n",
    "        self.window_size = config.window_size\n",
    "        self.spatial_merge_unit = self.spatial_merge_size * self.spatial_merge_size\n",
    "\n",
    "        self.patch_embed = Qwen2_5_VisionPatchEmbed(\n",
    "            patch_size=config.patch_size,\n",
    "            temporal_patch_size=config.temporal_patch_size,\n",
    "            in_channels=config.in_channels,\n",
    "            embed_dim=config.hidden_size,\n",
    "        )\n",
    "\n",
    "        head_dim = config.hidden_size // config.num_heads\n",
    "        self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Qwen2_5_VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]\n",
    "        )\n",
    "        self.merger = Qwen2_5_VLPatchMerger(\n",
    "            dim=config.out_hidden_size,\n",
    "            context_dim=config.hidden_size,\n",
    "            spatial_merge_size=config.spatial_merge_size,\n",
    "        )\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def rot_pos_emb(self, grid_thw):\n",
    "        pos_ids = []\n",
    "        for t, h, w in grid_thw:\n",
    "            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n",
    "            hpos_ids = hpos_ids.reshape(\n",
    "                h // self.spatial_merge_size,\n",
    "                self.spatial_merge_size,\n",
    "                w // self.spatial_merge_size,\n",
    "                self.spatial_merge_size,\n",
    "            )\n",
    "            hpos_ids = hpos_ids.permute(0, 2, 1, 3)\n",
    "            hpos_ids = hpos_ids.flatten()\n",
    "\n",
    "            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n",
    "            wpos_ids = wpos_ids.reshape(\n",
    "                h // self.spatial_merge_size,\n",
    "                self.spatial_merge_size,\n",
    "                w // self.spatial_merge_size,\n",
    "                self.spatial_merge_size,\n",
    "            )\n",
    "            wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n",
    "            wpos_ids = wpos_ids.flatten()\n",
    "            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n",
    "        pos_ids = torch.cat(pos_ids, dim=0)\n",
    "        max_grid_size = grid_thw[:, 1:].max()\n",
    "        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n",
    "        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n",
    "        return rotary_pos_emb\n",
    "\n",
    "    def get_window_index(self, grid_thw):\n",
    "        window_index: list = []\n",
    "        cu_window_seqlens: list = [0]\n",
    "        window_index_id = 0\n",
    "        vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size\n",
    "\n",
    "        for grid_t, grid_h, grid_w in grid_thw:\n",
    "            llm_grid_h, llm_grid_w = (\n",
    "                grid_h // self.spatial_merge_size,\n",
    "                grid_w // self.spatial_merge_size,\n",
    "            )\n",
    "            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(grid_t, llm_grid_h, llm_grid_w)\n",
    "            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size\n",
    "            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size\n",
    "            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size\n",
    "            num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size\n",
    "            index_padded = F.pad(index, (0, pad_w, 0, pad_h), \"constant\", -100)\n",
    "            index_padded = index_padded.reshape(\n",
    "                grid_t,\n",
    "                num_windows_h,\n",
    "                vit_merger_window_size,\n",
    "                num_windows_w,\n",
    "                vit_merger_window_size,\n",
    "            )\n",
    "            index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(\n",
    "                grid_t,\n",
    "                num_windows_h * num_windows_w,\n",
    "                vit_merger_window_size,\n",
    "                vit_merger_window_size,\n",
    "            )\n",
    "            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)\n",
    "            index_padded = index_padded.reshape(-1)\n",
    "            index_new = index_padded[index_padded != -100]\n",
    "            window_index.append(index_new + window_index_id)\n",
    "            cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n",
    "            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n",
    "            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n",
    "        window_index = torch.cat(window_index, dim=0)\n",
    "\n",
    "        return window_index, cu_window_seqlens\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):\n",
    "                The final hidden states of the model.\n",
    "            grid_thw (`torch.Tensor` of shape `(num_images_or_videos, 3)`):\n",
    "                The temporal, height and width of feature shape of each image in LLM.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`: hidden_states.\n",
    "        \"\"\"\n",
    "        hidden_states = self.patch_embed(hidden_states)\n",
    "        rotary_pos_emb = self.rot_pos_emb(grid_thw)\n",
    "        window_index, cu_window_seqlens = self.get_window_index(grid_thw)\n",
    "        cu_window_seqlens = torch.tensor(\n",
    "            cu_window_seqlens,\n",
    "            device=hidden_states.device,\n",
    "            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    "        )\n",
    "        cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)\n",
    "\n",
    "        seq_len, _ = hidden_states.size()\n",
    "        hidden_states = hidden_states.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "        hidden_states = hidden_states[window_index, :, :]\n",
    "        hidden_states = hidden_states.reshape(seq_len, -1)\n",
    "        rotary_pos_emb = rotary_pos_emb.reshape(seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n",
    "        rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n",
    "        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n",
    "        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n",
    "        position_embeddings = (emb.cos(), emb.sin())\n",
    "\n",
    "        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n",
    "            dim=0,\n",
    "            # Select dtype based on the following factors:\n",
    "            #  - FA2 requires that cu_seqlens_q must have dtype int32\n",
    "            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n",
    "            # See https://github.com/huggingface/transformers/pull/34852 for more information\n",
    "            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n",
    "        )\n",
    "        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n",
    "\n",
    "        for layer_num, blk in enumerate(self.blocks):\n",
    "            if layer_num in self.fullatt_block_indexes:\n",
    "                cu_seqlens_now = cu_seqlens\n",
    "            else:\n",
    "                cu_seqlens_now = cu_window_seqlens\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n",
    "        \n",
    "\n",
    "        hidden_states = self.merger(hidden_states)\n",
    "\n",
    "        reverse_indices = torch.argsort(window_index)\n",
    "        hidden_states = hidden_states[reverse_indices, :]\n",
    "\n",
    "\n",
    "        return hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc1970-2bd1-4b4c-864b-278997863d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838a4eb-240c-4e76-87ac-80fe3ff2ad47",
   "metadata": {},
   "source": [
    "### Video info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac112e-152b-4804-980b-291928efa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OpenCV\n",
    "import cv2\n",
    "\n",
    "# These paths are on Sockeye\n",
    "video_path_1 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/bIUmw2DVW7Q_9-3-rgb_front.mp4\"\n",
    "video_path_2 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a4Nxq0QV_WA_4-5-rgb_front.mp4\"\n",
    "video_path_3 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a5yNwUSiYpA_11-3-rgb_front.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path_1)\n",
    "\n",
    "# Get frame count\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Get FPS\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Calculate duration (seconds)\n",
    "duration = frame_count / fps\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(f\"Frame count: {frame_count}\")\n",
    "print(f\"fps: {fps}\")\n",
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786936f-89fc-43eb-9324-8a4be601d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using av since it was easier for me\n",
    "import av\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "container = av.open(video_path_1)\n",
    "\n",
    "video_stream = next(stream for stream in container.streams if stream.type == \"video\")\n",
    "\n",
    "frames_list = []\n",
    "for i, frame in enumerate(container.decode(video=0)):  # 0 -> first video stream\n",
    "    # frame is in PyAV's own format, so you often convert it to a NumPy array\n",
    "    frames_list.append(frame.to_image())  # shape: (height, width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e62a5-aa02-4549-a033-aec999876d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame list contains all the frames from the video\n",
    "frames_list[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c0d30-dd54-4d7d-8923-cec3c5464809",
   "metadata": {},
   "source": [
    "### QWen video pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e7b64-79fa-48ac-98cb-2af66e2b70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QWen video preprocess\n",
    "\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info\n",
    "video_path_1 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/bIUmw2DVW7Q_9-3-rgb_front.mp4\"\n",
    "video_path_2 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a4Nxq0QV_WA_4-5-rgb_front.mp4\"\n",
    "video_path_3 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a5yNwUSiYpA_11-3-rgb_front.mp4\"\n",
    "\n",
    "# Batch number is not explicitly specified\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": [{\n",
    "                    \"video\": video_path_1,\n",
    "                    \"min_pixels\": 4096, #128*128\n",
    "                    \"max_pixels\": 16384,  # 224*224\n",
    "                    \"fps\": 4\n",
    "                }]\n",
    "              },\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": [{\n",
    "                    \"video\": video_path_3,\n",
    "                    \"min_pixels\": 4096, #128*128\n",
    "                    \"max_pixels\": 16384,  # 224*224\n",
    "                    \"fps\": 4\n",
    "                }]\n",
    "              },\n",
    "            #{\"role\": \"user\", \"content\": [{\"video\": video_path_3}]}...]\n",
    "            ]\n",
    "\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bbe40-0ee9-420d-bd1c-9abb3394d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_kwargs contains the fps after pre-processing\n",
    "video_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781631e9-1de3-4d29-9305-cd6dce604b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processed output shape\n",
    "video_inputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6895b4f-8dc5-4621-9250-b9c6d1ce1f4a",
   "metadata": {},
   "source": [
    "#### helper fucntions for data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124154e-396b-42b5-b9db-83cc5630f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect the image/frames after preprocessing\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Convert the pixel representation number from 0-255 to 0-1\n",
    "adjusted_video_inputs = video_inputs[0]/255.0\n",
    "\n",
    "to_pil = T.ToPILImage()\n",
    "processed_images = []\n",
    "\n",
    "for i in range(adjusted_video_inputs.shape[0]):\n",
    "    # Convert each frame tensor to PIL Image\n",
    "    processed_image = to_pil(adjusted_video_inputs[i])\n",
    "    processed_images.append(processed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b2b84-43e8-44ba-893a-8a7755aef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display entire frame list\n",
    "from PIL import Image\n",
    "\n",
    "def display_images_side_by_side(image_list):\n",
    "    # Calculate the total width and the maximum height\n",
    "    total_width = sum(img.width for img in image_list)\n",
    "    max_height = max(img.height for img in image_list)\n",
    "    \n",
    "    # Create a new blank image with the calculated dimensions\n",
    "    result = Image.new('RGB', (total_width, max_height))\n",
    "    \n",
    "    # Paste each image next to each other\n",
    "    x_offset = 0\n",
    "    for img in image_list:\n",
    "        result.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Assuming processed_images is your list of PIL images\n",
    "combined_image = display_images_side_by_side(processed_images)\n",
    "\n",
    "# Display the combined image\n",
    "combined_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4cce1-3595-4a27-b78f-a5730c6fbf1a",
   "metadata": {},
   "source": [
    "#### QWen vision processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa96805-fd20-444a-8aa6-b9e9bca6a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLProcessor\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "inputs = processor(text = \"\", videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703cd9a-9a0d-4c1a-b34c-ef79e05bead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values_videos\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0cca7-9598-49d1-92d9-e8d8288a979d",
   "metadata": {},
   "source": [
    "### Run Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cde7d5-b6a3-4124-b4cd-ba5243070f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "from transformers import AutoConfig\n",
    "v_config = Qwen2_5_VLVisionConfig.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "                                                  torch_dtype = \"bfloat16\",\n",
    "                                                  #spatial_merge_size = 4\n",
    "                                                 )\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "#config\n",
    "\n",
    "#model = Qwen2_5_VisionTransformerPretrainedModel.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", config = v_config)\n",
    "\n",
    "model = Qwen2_5_VisionTransformerPretrainedModel(v_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d69d9-3940-41f8-8dd0-61257792837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device) \n",
    "inputs.to(device)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "       activities=[\n",
    "           torch.profiler.ProfilerActivity.CPU,\n",
    "           torch.profiler.ProfilerActivity.CUDA,\n",
    "       ],\n",
    "       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "       record_shapes=True,\n",
    "       profile_memory=True,\n",
    "       with_stack=True\n",
    "   ) as prof:\n",
    "    v_embedding = model.forward(inputs['pixel_values_videos'], inputs['video_grid_thw'])\n",
    "    \n",
    "    prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ab72b-dfe1-4e42-96c9-61dc26f510f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_embedding.shape\n",
    "\n",
    "# DIMENSIONS:\n",
    "\n",
    "# 1 video: INPUT: torch.Size([128, 1176]), OUTPUT: torch.Size([32, 2048])\n",
    "# 2 videos: INPUT: torch.Size([576, 1176]), OUTPUT: torch.Size([144, 2048])\n",
    "# 2 videos, merge size 4: INPUT: torch.Size([576, 1176]), OUTPUT: torch.Size([36, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9d9ba-17cc-4e5a-8c6b-49244da420dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "prof.export_memory_timeline(f\"1_video_4_fps_memory_check.html\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0b7df-2f0b-4e3a-a3ac-203c04fd1174",
   "metadata": {},
   "source": [
    "# Forward & Backward run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba03f0c-3a4f-4c69-9355-2668966e63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment class\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightning as pl\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import contrastive_encoders.encoders as encoders\n",
    "import contrastive_encoders.losses as losses\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "\n",
    "class VideoTextExp(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        video_encoder_cfg,\n",
    "        text_encoder_cfg,\n",
    "        #optimizer,\n",
    "        sample_rate: int = 16000,\n",
    "        initial_lr: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        num_warmup_steps: int = 0,\n",
    "        hard_negatives: bool = False,\n",
    "        tokenizer = None,\n",
    "        processor = None,\n",
    "        text = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        #print(\"text encoder cfg:\")\n",
    "        #print(self.hparams.text_encoder_cfg)\n",
    "        #print(self.hparams.video_encoder_cfg)\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", \n",
    "                                                      trust_remote_code=True)\n",
    "        \n",
    "        print(\"text_encoder initiated\")\n",
    "        \n",
    "        self.video_encoder = encoders.initialize_vision_encoder(self.hparams.video_encoder_cfg)\n",
    "        print(\"video_encoder initiated\")\n",
    "        \n",
    "        self.loss = losses.ContrastiveSigmoid\n",
    "        \n",
    "        # t and b for the loss function, should be set in the yaml file?\n",
    "        self.t_prime = torch.tensor(math.log(10))\n",
    "        self.b = torch.tensor(-10.0)\n",
    "        \n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.text = text \n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "        if tokenizer is not None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        if processor is not None:\n",
    "            self.processor = AutoProcessor.from_pretrained(processor)\n",
    "            \n",
    "        #self.zeroshot = zeroshot \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_params = [\n",
    "            {\"params\": self.video_encoder.parameters()},\n",
    "            {\"params\": self.text_encoder.parameters()}\n",
    "        ]\n",
    "        \n",
    "        # Using 8-bit adam\n",
    "        optimizer = bnb.optim.Adam8bit(model_params, \n",
    "                                       lr = self.hparams.initial_lr, \n",
    "                                       weight_decay = self.hparams.weight_decay)\n",
    "\n",
    "        max_steps = self.trainer.max_steps \n",
    "        \n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps = self.hparams.num_warmup_steps,\n",
    "            num_training_steps = max_steps\n",
    "        )\n",
    "        \n",
    "        return(\n",
    "            [optimizer],\n",
    "            [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, video_input, text_input):\n",
    "        text_features = self.text_encoder.encode(text_input, \n",
    "                                                 task=self.hparams.text_encoder_cfg[\"task\"], \n",
    "                                                 truncate_dim=self.hparams.text_encoder_cfg[\"out_hidden_size\"])\n",
    "        text_features = torch.tensor(text_features)\n",
    "        \n",
    "        video_features = self.encode_video(video_input) \n",
    "        return video_features, text_features\n",
    "\n",
    "    def encode_video(self, video_input):\n",
    "        pixel_values = video_input['pixel_values_videos']\n",
    "        grid_thw = video_input['video_grid_thw']\n",
    "        \n",
    "        video_features = self.video_encoder(pixel_values, grid_thw)\n",
    "\n",
    "        return video_features\n",
    "\n",
    "    def encode_text(self, text_input):\n",
    "        text_features = self.text_encoder(**text_input)\n",
    "        return text_features\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        video_input, text_input = batch\n",
    "        video_features, text_features = self.forward(video_input, text_input)\n",
    "        loss = self.loss(video_features, \n",
    "                         text_features, \n",
    "                         self.t_prime,\n",
    "                         self.b\n",
    "                        )\n",
    "    \n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video_input, text_input = batch\n",
    "        video_features, text_features = self.forward(video_input, text_input)\n",
    "        loss = self.loss(video_features, \n",
    "                         text_features, \n",
    "                         self.t_prime,\n",
    "                         self.b\n",
    "                        )\n",
    "        \n",
    "        self.validation_step_outputs.append(loss)\n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        if self.global_rank == 0:\n",
    "            avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step_outputs]).mean()\n",
    "            self.log(\"val_loss\", avg_loss, sync_dist = True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf6bc7-7f7f-4d37-a7d0-97ca6d5951b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "import yaml\n",
    "from transformers import AutoConfig\n",
    "\n",
    "with open(\"config/base.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "text_encoder_cfg = config[\"model\"][\"init_args\"][\"text_encoder_cfg\"]\n",
    "video_encoder_cfg = config[\"model\"][\"init_args\"][\"video_encoder_cfg\"]\n",
    "\n",
    "\n",
    "#text_encoder_cfg = AutoConfig.from_pretrained(\"jinaai/jina-embeddings-v3\")\n",
    "#video_encoder_cfg = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "# Pass the loaded config as arguments to the model\n",
    "model = VideoTextExp(text_encoder_cfg=text_encoder_cfg, video_encoder_cfg=video_encoder_cfg)\n",
    "\n",
    "# Print to verify\n",
    "#print(model.hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb51a4-a364-4ad3-827a-b62333aa1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Data preprocessing\n",
    "example_text = [\n",
    "    \"This is an example sentence.\",\n",
    "    \"this short sentence.\",\n",
    "    #\"I have an extrememly long sentence that I want to test the text-encoder on.\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2VLProcessor\n",
    "\n",
    "video_path_1 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/bIUmw2DVW7Q_9-3-rgb_front.mp4\"\n",
    "video_path_2 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a4Nxq0QV_WA_4-5-rgb_front.mp4\"\n",
    "video_path_3 = \"/scratch/st-jzhu71-1/ewong25/my_jupyter/How2Sign_Clips/val/a5yNwUSiYpA_11-3-rgb_front.mp4\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": [{\n",
    "                    \"video\": video_path_2,\n",
    "                    \"min_pixels\": 4096, #128*128\n",
    "                    \"max_pixels\": 16384,  # 224*224\n",
    "                }]\n",
    "              },\n",
    "            # {\"role\": \"user\", \n",
    "            #  \"content\": [{\n",
    "            #         \"video\": video_path_3,\n",
    "            #         \"min_pixels\": 4096, #128*128\n",
    "            #         \"max_pixels\": 16384,  # 224*224\n",
    "            #     }]\n",
    "            #   }\n",
    "            #{\"role\": \"user\", \"content\": [{\"video\": video_path_3}]}]\n",
    "            ]\n",
    "\n",
    "\n",
    "# Video Preprocessing\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "v_inputs = processor(text = \"\", videos=video_inputs, padding=True, return_tensors=\"pt\", mergesize = 4)\n",
    "v_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185bb81-529b-4952-90c3-a8edf62e80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "v_inputs.to(device)\n",
    "example_text.to(device)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "       activities=[\n",
    "           torch.profiler.ProfilerActivity.CPU,\n",
    "           torch.profiler.ProfilerActivity.CUDA,\n",
    "       ],\n",
    "       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),\n",
    "       record_shapes=True,\n",
    "       profile_memory=True,\n",
    "       with_stack=True\n",
    "   ) as prof:\n",
    "    \n",
    "    video_features, text_features = model.forward(v_inputs, example_text)\n",
    "    loss = model.loss(video_features, text_features, model.t_prime, model.b)\n",
    "    loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
